{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Exploration for Recurrent Shading Estimator\n",
    "\n",
    "This file showcases an analysis of how deep learning would be implemeneted in this\n",
    "code with respect to the model recurrent shading estimator. Though deep learning\n",
    "is mentioned in our writeup, it is further detailed in this file as an exploration\n",
    "of different model representations and interpretations.\n",
    "\n",
    "Our objective for the shader is to find a coefficient value to best shade our bids\n",
    "as defined by the waterfall algorithm; that is to say, we aim to bid high enough \n",
    "such that we are able to beat out bidders to attain the reach for a given campaign, \n",
    "but also low enough such that we aren't overpaying and losing money that we could \n",
    "be gaining. \n",
    "\n",
    "Respectively, we outline our problem mathematically: Let f(x) be a convex decreasing\n",
    "piecewise function represent a mapping from bid shading to index of impression claimed \n",
    "with all bidders other than the player agent. Let $f^{-1}(x)$ represent a mapping from\n",
    "number of index of impression claimed to bid shading. Let us define $H$ as the set of\n",
    "all hypothesis classes (or RNN parameter configurations, as per our implementation) that\n",
    "represent a mapping from index of impression and state information to bid shading.\n",
    "For a given segment, we assume that there are $n$ total impressions and our campaign has a\n",
    "reach target of $R$.\n",
    "\n",
    "Our objective function is as follows:\n",
    "\n",
    "$$\\text{argmin}_{h \\in H} H(x, S)$$\n",
    "$$\\text{s.t.} \\;\\; H(n - R) > f^{-1}(n - R)$$\n",
    "\n",
    "Which is to say we aim to bid just above what other players would have bid such that\n",
    "we can win at least the last R bids. This ensures that we do not overpay but also\n",
    "achieve our reach. Note that we do not have access to the true distribution f(x) and\n",
    "instead treat H(x, S) as an intermediate approximator for the function $f^{-1}(x)$.\n",
    "\n",
    "A common point for both of these is the question of state featurization. Our proposed\n",
    "methodology was to store known information (notably quality score, budget, and known\n",
    "campaigns) inside a vector. An autoencoder could be trained to represent these\n",
    "states as a vectorized low-dimensional state, then this intermediate representation\n",
    "would be used as an input into the recurrent network.\n",
    "\n",
    "We now consider two approaches which drastically affect how we deal utilize the\n",
    "objective function: supervised learning and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Supervised Learning\n",
    "\n",
    "In supervised learning scenarios (namely regression, since our goal is to approximate)\n",
    "a continuous value, we create a function of our features to approximate the true labels. \n",
    "This assumes that we have some knowledge of the true labels. We are able to generate these\n",
    "labels by running simulations of TA bots against each other, then capturing where each\n",
    "threshold value is.\n",
    "\n",
    "However, note that this presupposes access to simulation, or rather also the ability to\n",
    "run other bots against each other in a training loop. Since we do not directly have\n",
    "access to the implementations of other users, we would only be able to train against\n",
    "features and labels from training loops with the provided TA bots.\n",
    "\n",
    "Adversarial policy heavily affects the training process. Since supervised learning\n",
    "attempts to create a model that represents the true distribution $f^{-1}$, empirical\n",
    "risk minimization will not yield meaningful results because we are training against/\n",
    "learning the wrong distribution.\n",
    "\n",
    "This implies that as it stands, the supervised learning approach is limited by\n",
    "our access to other bots as training examples. Nevertheless, we detail an approach\n",
    "to training against other bots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adx.tier1_ndays_ncampaign_agent import Tier1NDaysNCampaignsAgent\n",
    "from adx.adx_game_simulator import AdXGameSimulator\n",
    "import random\n",
    "from adx.tier1_ndays_ncampaign_agent import Tier1NDaysNCampaignsAgent\n",
    "from adx.agents import NDaysNCampaignsAgent\n",
    "from math import atan\n",
    "from adx.structures import MarketSegment\n",
    "\n",
    "# We first modifiy the AdXGameSimulator class to include a new method run_simulation.\n",
    "# This method will run the simulation for a given number of times and collect the bids\n",
    "# for each agent at any day, as well as the population for that given market segment.\n",
    "class ModifiedAdXGameSimulator(AdXGameSimulator):\n",
    "    def calculate_effective_reach(self, x: int, R: int) -> float:\n",
    "        return (2.0 / 4.08577) * (atan(4.08577 * ((x + 0.0) / R) - 3.08577) - atan(-3.08577))\n",
    "\n",
    "    # This time, we will run the simulation for a given number of times and collect the bids\n",
    "    # for each agent at any day, as well as the population for that given market segment.\n",
    "    def run_simulation(self, agents: list[NDaysNCampaignsAgent], num_simulations: int, target_segment : MarketSegment) -> None:\n",
    "        total_agent_bids = {agent : [] for agent in agents} # ADDED, THESE ARE THE BIDS FOR EACH AGENT\n",
    "        total_profits = {agent : 0.0 for agent in agents}\n",
    "        for i in range(num_simulations):\n",
    "            self.states = self.init_agents(agents)\n",
    "            self.campaigns = dict()\n",
    "            # Initialize campaigns \n",
    "            for agent in self.agents:    \n",
    "                    agent.current_game = i + 1 \n",
    "                    agent.my_campaigns = set()\n",
    "                    random_campaign = self.generate_campaign(start_day=1)\n",
    "                    agent_state = self.states[agent]\n",
    "                    random_campaign.budget = random_campaign.reach\n",
    "                    agent_state.add_campaign(random_campaign)\n",
    "                    agent.my_campaigns.add(random_campaign)\n",
    "                    self.campaigns[random_campaign.uid] = random_campaign\n",
    "\n",
    "            for day in range(1, self.num_days + 1):\n",
    "                for agent in self.agents:\n",
    "                    agent.current_day = day\n",
    "\n",
    "                # Generate new campaigns and filter\n",
    "                if day + 1 < self.num_days + 1:\n",
    "                    new_campaigns = [self.generate_campaign(start_day=day + 1) for _ in range(self.campaigns_per_day)]\n",
    "                    new_campaigns = [c for c in new_campaigns if c.end_day <= self.num_days]\n",
    "                    # Solicit campaign bids and run campaign auctions\n",
    "                    agent_bids = dict()\n",
    "                    for agent in self.agents:\n",
    "                        agent_bids[agent] = agent.get_campaign_bids(new_campaigns)\n",
    "\n",
    "                # Solicit ad bids from agents and run ad auctions\n",
    "                ad_bids = []\n",
    "                for agent in self.agents:\n",
    "                    ad_bids.extend(agent.get_ad_bids())\n",
    "                users = self.generate_auction_items(10000)\n",
    "                self.run_ad_auctions(ad_bids, users, day)\n",
    "\n",
    "                # Update campaign states, quality scores, and profits\n",
    "                for agent in self.agents:\n",
    "                    agent_state = self.states[agent]\n",
    "                    todays_profit = 0.0\n",
    "                    new_qs_count = 0\n",
    "                    new_qs_val = 0.0\n",
    "\n",
    "                    for campaign in agent_state.campaigns.values():\n",
    "                        if campaign.start_day <= day <= campaign.end_day:\n",
    "                            if day == campaign.end_day:\n",
    "                                impressions = agent_state.impressions[campaign.uid]\n",
    "                                total_cost = agent_state.spend[campaign.uid]\n",
    "                                effective_reach = self.calculate_effective_reach(impressions, campaign.reach)\n",
    "                                todays_profit += (effective_reach) * agent_state.budgets[campaign.uid] - total_cost\n",
    "\n",
    "                                new_qs_count += 1\n",
    "                                new_qs_val += effective_reach\n",
    "\n",
    "                    if new_qs_count > 0:\n",
    "                        new_qs_val /= new_qs_count\n",
    "                        self.states[agent].quality_score = (1 - self.α) * self.states[agent].quality_score + self.α * new_qs_val\n",
    "                        agent.quality_score = self.states[agent].quality_score\n",
    "\n",
    "                    agent_state.profits += todays_profit\n",
    "                    agent.profit += todays_profit\n",
    "                \n",
    "                ## ADDED : WE SAVE THE BIDS FOR EACH AGENT\n",
    "                for agent in self.agents:\n",
    "                    if self.states[agent].campaigns.intersection(target_segment):\n",
    "                        total_agent_bids[agent].append(agent_bids[agent])\n",
    "\n",
    "                # Run campaign auctions\n",
    "                self.run_campaign_auctions(agent_bids, new_campaigns)\n",
    "                # Run campaign endowments\n",
    "                for agent in self.agents:\n",
    "                    if random.random() < min(1, agent.quality_score):\n",
    "                        random_campaign = self.generate_campaign(start_day=day)\n",
    "                        agent_state = self.states[agent]\n",
    "                        random_campaign.budget = random_campaign.reach\n",
    "                        agent_state.add_campaign(random_campaign)\n",
    "                        agent.my_campaigns.add(random_campaign)\n",
    "                        self.campaigns[random_campaign.uid] = random_campaign\n",
    "\n",
    "        # PRINTING OMITTED\n",
    "        #     for agent in self.agents:\n",
    "        #         total_profits[agent] += self.states[agent].profits \n",
    "        #     self.print_game_results()\n",
    "        # self.print_final_results(total_profits, num_simulations)\n",
    "\n",
    "        return total_agent_bids\n",
    "\n",
    "# 10 sample agents. \n",
    "test_agents = [Tier1NDaysNCampaignsAgent(name=f\"Agent {i + 1}\") for i in range(10)]\n",
    "simulator = ModifiedAdXGameSimulator()\n",
    "\n",
    "# Arbitrarily we pick male, young, lowincome. We could apply this to all market segments and\n",
    "# generalize accoridngly.\n",
    "sampleSegment = MarketSegment((\"Male\", \"Young\", \"LowIncome\"))\n",
    "bids = simulator.run_simulation(test_agents, 10, sampleSegment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adx.adx_game_simulator import CONFIG\n",
    "\n",
    "# Optimal shading based on the aforementioned waterfall algorithm.\n",
    "def optimal_shading(bids: list[float], segment : MarketSegment) -> list[float]:\n",
    "    expected_population = CONFIG[\"market_segment_pop\"][segment]\n",
    "    total_bids = [sum(bid) for bid in zip(*bids)]\n",
    "    return [bid / expected_population for bid in total_bids]\n",
    "\n",
    "bids = [optimal_shading(bids, sampleSegment) for bids in bids.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our derived bids and populations, we are consequently able to run a training \n",
    "loop that trains our recurrent network on this information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Reinforcement Learning\n",
    "\n",
    "Reinforcement learning addresses some issues raised by the supervised learning approach,\n",
    "but also raises some problems of its own. We treat the problem as a POMDP, since other\n",
    "people's valuations and overall campaign informations are unknown to us. Accordingly,\n",
    "we can only use previous information from previous days, notably how many impressions\n",
    "we were able to score in any given MarketSegment.\n",
    "\n",
    "We considered self-play as an option. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
